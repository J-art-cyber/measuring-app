import streamlit as st
import pandas as pd
import gspread
import json
import re
import io
from oauth2client.service_account import ServiceAccountCredentials
from datetime import datetime

st.set_page_config(page_title="æ¡å¯¸ãƒ‡ãƒ¼ã‚¿ç®¡ç†", layout="wide")
page = st.sidebar.selectbox("ãƒšãƒ¼ã‚¸ã‚’é¸æŠ", [
    "æ¡å¯¸å…¥åŠ›", "æ¡å¯¸æ¤œç´¢", "å•†å“ã‚¤ãƒ³ãƒãƒ¼ãƒˆ", "åŸºæº–å€¤ã‚¤ãƒ³ãƒãƒ¼ãƒˆ", "æ¡å¯¸ãƒ˜ãƒƒãƒ€ãƒ¼åˆæœŸåŒ–", "ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ç®¡ç†"
])

scope = ["https://spreadsheets.google.com/feeds", "https://www.googleapis.com/auth/drive"]
json_key = json.loads(st.secrets["GOOGLE_CREDENTIALS"])
creds = ServiceAccountCredentials.from_json_keyfile_dict(json_key, scope)
client = gspread.authorize(creds)
spreadsheet = client.open("æ¡å¯¸ç®¡ç†ãƒ‡ãƒ¼ã‚¿")

ideal_order_dict = {
    "ã‚¸ãƒ£ã‚±ãƒƒãƒˆ": ["è‚©å¹…", "èƒ¸å¹…", "èƒ´å›²", "è¢–ä¸ˆ", "ç€ä¸ˆ"],
    "ãƒ‘ãƒ³ãƒ„": ["ã‚¦ã‚¨ã‚¹ãƒˆ", "è‚¡ä¸Š", "è‚¡ä¸‹", "ãƒ¯ã‚¿ãƒª", "è£¾å¹…"],
    "ãƒ€ã‚¦ãƒ³": ["è‚©å¹…", "èƒ¸å¹…", "è¢–ä¸ˆ", "ç€ä¸ˆ", "è¥Ÿé«˜"],
    "ãƒ–ãƒ«ã‚¾ãƒ³": ["è‚©å¹…", "èƒ¸å¹…", "è¢–ä¸ˆ", "ç€ä¸ˆ", "è¥Ÿé«˜"],
    "ã‚³ãƒ¼ãƒˆ": ["è‚©å¹…", "èƒ¸å¹…", "è¢–ä¸ˆ", "ç€ä¸ˆ", "è¥Ÿé«˜"],
    "ãƒ‹ãƒƒãƒˆ": ["è‚©å¹…", "èƒ¸å¹…", "è¢–ä¸ˆ", "ç€ä¸ˆ"],
    "ã‚«ãƒƒãƒˆã‚½ãƒ¼": ["è‚©å¹…", "èƒ¸å¹…", "è¢–ä¸ˆ", "ç€ä¸ˆ"],
    "ãƒ¬ã‚¶ãƒ¼": ["è‚©å¹…", "èƒ¸å¹…", "è¢–ä¸ˆ", "ç€ä¸ˆ", "è¥Ÿé«˜"],
    "é´": ["å…¨é•·", "æœ€å¤§å¹…"],
    "å·»ç‰©": ["å…¨é•·", "æ¨ªå¹…"],
    "å°ç‰©ãƒ»ãã®ä»–": ["é ­å‘¨ã‚Š", "ãƒ„ãƒ", "é«˜ã•", "æ¨ªå¹…", "ãƒãƒ"],
    "ã‚·ãƒ£ãƒ„": ["è‚©å¹…", "è£„ä¸ˆ", "èƒ¸å¹…", "èƒ´å›²", "è¢–ä¸ˆ", "ç€ä¸ˆ"],
    "ã‚·ãƒ£ãƒ„ã‚¸ãƒ£ã‚±ãƒƒãƒˆ": ["è‚©å¹…", "èƒ¸å¹…", "è¢–ä¸ˆ", "ç€ä¸ˆ"],
    "ã‚¹ãƒ¼ãƒ„": ["è‚©å¹…", "èƒ¸å¹…", "èƒ´å›²", "è¢–ä¸ˆ", "ç€ä¸ˆ", "ã‚¦ã‚¨ã‚¹ãƒˆ", "è‚¡ä¸Š", "è‚¡ä¸‹", "ãƒ¯ã‚¿ãƒª", "è£¾å¹…"],
    "ãƒ™ãƒ«ãƒˆ": ["å…¨é•·", "ãƒ™ãƒ«ãƒˆå¹…"],
    "åŠè¢–": ["è‚©å¹…", "èƒ¸å¹…", "è¢–ä¸ˆ", "å‰ä¸ˆ", "å¾Œä¸ˆ"]
}

if page == "æ¡å¯¸å…¥åŠ›":
    st.title("ğŸ“± æ¡å¯¸å…¥åŠ›ï¼ˆæ¨ªä¸¦ã³ï¼šã‚¹ãƒãƒ›ãƒ»PCå…¼ç”¨ï¼‰")
    custom_orders = {
        "ãƒ‘ãƒ³ãƒ„": ["ã‚¦ã‚¨ã‚¹ãƒˆ", "è‚¡ä¸Š", "ãƒ¯ã‚¿ãƒª", "è‚¡ä¸‹", "è£¾å¹…"],
        "ã‚·ãƒ£ãƒ„": ["è‚©å¹…", "èƒ¸å¹…", "èƒ´å›²", "è£„ä¸ˆ", "è¢–ä¸ˆ", "ç€ä¸ˆ"]
    }

    master_df = pd.DataFrame(spreadsheet.worksheet("å•†å“ãƒã‚¹ã‚¿").get_all_records())
    template_df = pd.DataFrame(spreadsheet.worksheet("æ¡å¯¸ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ").get_all_records())
    result_df = pd.DataFrame(spreadsheet.worksheet("æ¡å¯¸çµæœ").get_all_records())
    archive_df = pd.DataFrame(spreadsheet.worksheet("æ¡å¯¸ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–").get_all_records())
    combined_df = pd.concat([result_df, archive_df], ignore_index=True)

    brand_list = master_df["ãƒ–ãƒ©ãƒ³ãƒ‰"].dropna().unique().tolist()
    selected_brand = st.selectbox("ãƒ–ãƒ©ãƒ³ãƒ‰ã‚’é¸æŠ", brand_list)
    filtered_df = master_df[master_df["ãƒ–ãƒ©ãƒ³ãƒ‰"] == selected_brand]

    pid_list = filtered_df["ç®¡ç†ç•ªå·"].dropna().unique().tolist()
    selected_pid = st.selectbox("ç®¡ç†ç•ªå·ã‚’é¸æŠ", pid_list)
    product_group = filtered_df[filtered_df["ç®¡ç†ç•ªå·"] == selected_pid]
    product_row = product_group.iloc[0]
    category = product_row["ã‚«ãƒ†ã‚´ãƒª"]

    st.write(f"**å•†å“åï¼š** {product_row['å•†å“å']}ã€€ã€€**ã‚«ãƒ©ãƒ¼ï¼š** {product_row['ã‚«ãƒ©ãƒ¼']}")
    sizes = product_group["ã‚µã‚¤ã‚º"].tolist()

    template_row = template_df[template_df["ã‚«ãƒ†ã‚´ãƒª"] == category]
    if template_row.empty:
        st.warning("ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        st.stop()

    raw_items = template_row.iloc[0]["æ¡å¯¸é …ç›®"].replace("ã€", ",").split(",")
    all_items = [re.sub(r'ï¼ˆ.*?ï¼‰', '', i).strip() for i in raw_items if i.strip()]
    custom_order = custom_orders.get(category, [])
    items = [i for i in custom_order if i in all_items] + [i for i in all_items if i not in custom_order]

    data = {item: [] for item in items}
    remarks = []
    for size in sizes:
        row = combined_df[(combined_df["å•†å“ç®¡ç†ç•ªå·"] == selected_pid) & (combined_df["ã‚µã‚¤ã‚º"] == size)]
        for item in items:
            val = row[item].values[0] if not row.empty and item in row.columns else ""
            data[item].append(val)
        note = row["å‚™è€ƒ"].values[0] if not row.empty and "å‚™è€ƒ" in row.columns else ""
        remarks.append(note)
    data["å‚™è€ƒ"] = remarks
    df = pd.DataFrame(data, index=sizes)
    df.index.name = "ã‚µã‚¤ã‚º"

    # -------------------------
    # åŸºæº–å€¤ã®è¡¨ç¤ºï¼ˆä»£è¡¨IDãƒ™ãƒ¼ã‚¹ï¼‰
    # -------------------------
    try:
        base_master_df = pd.DataFrame(spreadsheet.worksheet("åŸºæº–IDãƒã‚¹ã‚¿").get_all_records())
        standard_df = pd.DataFrame(spreadsheet.worksheet("åŸºæº–å€¤").get_all_records())

        # ç®¡ç†ç•ªå·ã‹ã‚‰ä»£è¡¨IDã‚’ç‰¹å®š
        base_row = base_master_df[base_master_df["å•†å“ç®¡ç†ç•ªå·"] == selected_pid]
        if not base_row.empty:
            base_id = base_row.iloc[0]["åŸºæº–ID"]
            st.markdown(f"### ğŸ“ åŸºæº–å€¤ï¼ˆåŸºæº–ID: {base_id}ï¼‰")

            filtered_standard = standard_df[standard_df["åŸºæº–ID"] == base_id]
            if not filtered_standard.empty:
                filtered_standard = filtered_standard.drop(columns=["åŸºæº–ID"])
                filtered_standard = filtered_standard.set_index("ã‚µã‚¤ã‚º")
                st.dataframe(filtered_standard, use_container_width=True)
            else:
                st.info("ã“ã®åŸºæº–IDã«å¯¾å¿œã™ã‚‹åŸºæº–å€¤ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
        else:
            st.info("ã“ã®å•†å“ã«ã¯åŸºæº–IDãŒç´ã¥ã„ã¦ã„ã¾ã›ã‚“ã€‚")
    except Exception as e:
        st.warning(f"åŸºæº–å€¤ã®è¡¨ç¤ºã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")


    st.markdown("### æ¡å¯¸å€¤ã¨å‚™è€ƒã®å…¥åŠ›ï¼ˆç›´æ¥ç·¨é›†ï¼‰")
    edited_df = st.data_editor(df, use_container_width=True, num_rows="dynamic")


    if st.button("ä¿å­˜ã™ã‚‹"):
        result_sheet = spreadsheet.worksheet("æ¡å¯¸çµæœ")
        headers = result_sheet.row_values(1)
        master_sheet = spreadsheet.worksheet("å•†å“ãƒã‚¹ã‚¿")
        full_master_df = pd.DataFrame(master_sheet.get_all_records())

        saved_sizes = []

        for size in edited_df.index:
            size_str = str(size).strip()
            if not size_str:
                continue
            if edited_df.loc[size, items].replace("", float("nan")).isna().all():
                continue

            save_data = {
                "æ—¥ä»˜": datetime.now().strftime("%Y-%m-%d"),
                "å•†å“ç®¡ç†ç•ªå·": selected_pid,
                "ãƒ–ãƒ©ãƒ³ãƒ‰": selected_brand,
                "ã‚«ãƒ†ã‚´ãƒª": category,
                "å•†å“å": product_row["å•†å“å"],
                "ã‚«ãƒ©ãƒ¼": product_row["ã‚«ãƒ©ãƒ¼"],
                "ã‚µã‚¤ã‚º": size_str,
                "å‚™è€ƒ": edited_df.loc[size, "å‚™è€ƒ"]
            }
            for item in items:
                save_data[item] = edited_df.loc[size, item]

            new_row = [save_data.get(h, "") for h in headers]
            result_sheet.append_row(new_row)
            saved_sizes.append(size_str)

        updated_master_df = full_master_df[~(
            (full_master_df["ç®¡ç†ç•ªå·"] == selected_pid) &
            (full_master_df["ã‚µã‚¤ã‚º"].isin(saved_sizes))
        )]
        master_sheet.clear()
        master_sheet.update([updated_master_df.columns.tolist()] + updated_master_df.values.tolist())

        st.success("âœ… æ¡å¯¸ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜ã—ã€å•†å“ãƒã‚¹ã‚¿ã‹ã‚‰è©²å½“ã‚µã‚¤ã‚ºã‚’å‰Šé™¤ã—ã¾ã—ãŸã€‚")
        st.rerun()

    st.markdown("### ğŸ‘• åŒã˜ãƒ¢ãƒ‡ãƒ«ã®éå»æ¡å¯¸ãƒ‡ãƒ¼ã‚¿ï¼ˆæ¯”è¼ƒç”¨ï¼‰")
    try:
        model_prefix = selected_pid[:8]
        model_df = combined_df[
            (combined_df["å•†å“ç®¡ç†ç•ªå·"].str[:8] == model_prefix) &
            (combined_df["å•†å“ç®¡ç†ç•ªå·"] != selected_pid)
        ]
        base_cols = ["æ—¥ä»˜", "å•†å“ç®¡ç†ç•ªå·", "ã‚µã‚¤ã‚º"]
        show_cols = base_cols + [col for col in model_df.columns if col in items]
        show_df = model_df[show_cols].sort_values(by=["æ—¥ä»˜", "ã‚µã‚¤ã‚º"], ascending=[False, True])
        st.dataframe(show_df, use_container_width=True)
    except Exception as e:
        st.warning(f"åŒãƒ¢ãƒ‡ãƒ«æ¡å¯¸ãƒ‡ãƒ¼ã‚¿ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")

    st.markdown("### ğŸ“… æœ¬æ—¥ç™»éŒ²ã—ãŸæ¡å¯¸ãƒ‡ãƒ¼ã‚¿ä¸€è¦§")
    today_str = datetime.now().strftime("%Y-%m-%d")
    try:
        today_df = combined_df[combined_df["æ—¥ä»˜"] == today_str]
        if not today_df.empty:
            base_cols = ["å•†å“ç®¡ç†ç•ªå·", "ã‚µã‚¤ã‚º"]
            show_cols = base_cols + [col for col in today_df.columns if col in items]
            show_df = today_df[show_cols].sort_values(by=["å•†å“ç®¡ç†ç•ªå·", "ã‚µã‚¤ã‚º"])
            st.dataframe(show_df, use_container_width=True)
        else:
            st.info("ä»Šæ—¥ã¯ã¾ã æ¡å¯¸ãƒ‡ãƒ¼ã‚¿ãŒç™»éŒ²ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
    except Exception as e:
        st.warning(f"ä»Šæ—¥ã®æ¡å¯¸ãƒ‡ãƒ¼ã‚¿ã‚’è¡¨ç¤ºã§ãã¾ã›ã‚“ã§ã—ãŸ: {e}")




# ---------------------
# æ¡å¯¸æ¤œç´¢ãƒšãƒ¼ã‚¸ï¼ˆã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ã¨çµ±åˆæ¤œç´¢ï¼‹ãƒ–ãƒ©ãƒ³ãƒ‰é€£å‹•ã§ç®¡ç†ç•ªå·ãƒ»ã‚µã‚¤ã‚ºãƒ»ã‚«ãƒ†ã‚´ãƒªã‚’çµã‚‹ï¼‰
# ---------------------
elif page == "æ¡å¯¸æ¤œç´¢":
    st.title("ğŸ” æ¡å¯¸çµæœæ¤œç´¢")
    try:
        result_values = spreadsheet.worksheet("æ¡å¯¸çµæœ").get_all_values()
        archive_values = spreadsheet.worksheet("æ¡å¯¸ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–").get_all_values()

        def to_df(values):
            if not values:
                return pd.DataFrame()
            headers = values[0]
            data = [row + [''] * (len(headers) - len(row)) for row in values[1:]]
            return pd.DataFrame(data, columns=headers)

        result_df = to_df(result_values)
        archive_df = to_df(archive_values)
        combined_df = pd.concat([result_df, archive_df], ignore_index=True)

        # ãƒ–ãƒ©ãƒ³ãƒ‰é¸æŠ
        selected_brands = st.multiselect("ğŸ”¸ ãƒ–ãƒ©ãƒ³ãƒ‰ã‚’é¸æŠ", sorted(combined_df["ãƒ–ãƒ©ãƒ³ãƒ‰"].dropna().unique()))

        # ãƒ–ãƒ©ãƒ³ãƒ‰ã«åŸºã¥ããƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
        if selected_brands:
            filtered_df = combined_df[combined_df["ãƒ–ãƒ©ãƒ³ãƒ‰"].isin(selected_brands)]
            pid_options = sorted(filtered_df["å•†å“ç®¡ç†ç•ªå·"].dropna().unique())
            size_options = sorted(filtered_df["ã‚µã‚¤ã‚º"].dropna().unique())
            category_options = sorted(filtered_df["ã‚«ãƒ†ã‚´ãƒª"].dropna().unique())
        else:
            pid_options = sorted(combined_df["å•†å“ç®¡ç†ç•ªå·"].dropna().unique())
            size_options = sorted(combined_df["ã‚µã‚¤ã‚º"].dropna().unique())
            category_options = sorted(combined_df["ã‚«ãƒ†ã‚´ãƒª"].dropna().unique())

        # ç®¡ç†ç•ªå·ãƒ»ã‚µã‚¤ã‚ºãƒ»ã‚«ãƒ†ã‚´ãƒªã‚’é¸æŠè‚¢è¡¨ç¤º
        selected_pids = st.multiselect("ğŸ”¹ ç®¡ç†ç•ªå·ã‚’é¸æŠ", pid_options)
        selected_sizes = st.multiselect("ğŸ”º ã‚µã‚¤ã‚ºã‚’é¸æŠ", size_options)
        keyword = st.text_input("ğŸ” ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã§æ¤œç´¢ï¼ˆå•†å“åã€ç®¡ç†ç•ªå·ãªã©ï¼‰")
        category_filter = st.selectbox("ğŸ“‚ ã‚«ãƒ†ã‚´ãƒªã§è¡¨ç¤ºé …ç›®ã‚’çµã‚‹", ["ã™ã¹ã¦è¡¨ç¤º"] + category_options)

        # æ¡ä»¶ã«å¿œã˜ã¦ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
        filtered_df = combined_df.copy()
        if selected_brands:
            filtered_df = filtered_df[filtered_df["ãƒ–ãƒ©ãƒ³ãƒ‰"].isin(selected_brands)]
        if selected_pids:
            filtered_df = filtered_df[filtered_df["å•†å“ç®¡ç†ç•ªå·"].isin(selected_pids)]
        if selected_sizes:
            filtered_df = filtered_df[filtered_df["ã‚µã‚¤ã‚º"].isin(selected_sizes)]
        if keyword:
            filtered_df = filtered_df[filtered_df.apply(lambda row: keyword.lower() in str(row.values).lower(), axis=1)]
        if category_filter != "ã™ã¹ã¦è¡¨ç¤º":
            filtered_df = filtered_df[filtered_df["ã‚«ãƒ†ã‚´ãƒª"] == category_filter]

        # è¡¨ç¤ºåˆ—ã®ä¸¦ã³æ›¿ãˆ
        base_cols = ["æ—¥ä»˜", "å•†å“ç®¡ç†ç•ªå·", "ãƒ–ãƒ©ãƒ³ãƒ‰", "ã‚«ãƒ†ã‚´ãƒª", "å•†å“å", "ã‚«ãƒ©ãƒ¼", "ã‚µã‚¤ã‚º"]
        ideal_cols = ideal_order_dict.get(category_filter, [])
        ordered_cols = base_cols + [col for col in ideal_cols if col in filtered_df.columns] + \
                       [col for col in filtered_df.columns if col not in base_cols + ideal_cols]
        filtered_df = filtered_df[ordered_cols]
        filtered_df = filtered_df.loc[:, ~((filtered_df == "") | (filtered_df.isna())).all(axis=0)]

        # æ¤œç´¢çµæœè¡¨ç¤º
        st.write(f"ğŸ” æ¤œç´¢çµæœ: {len(filtered_df)} ä»¶")
        st.dataframe(filtered_df, use_container_width=True)

        # Excelå‡ºåŠ›
        if not filtered_df.empty:
            to_excel = io.BytesIO()
            with pd.ExcelWriter(to_excel, engine="openpyxl") as writer:
                filtered_df.to_excel(writer, index=False, sheet_name="æ¡å¯¸çµæœ")
                writer.sheets["æ¡å¯¸çµæœ"].auto_filter.ref = writer.sheets["æ¡å¯¸çµæœ"].dimensions
            to_excel.seek(0)

            st.download_button(
                label="ğŸ“¥ æ¤œç´¢çµæœã‚’Excelã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                data=to_excel,
                file_name="æ¡å¯¸çµæœ_æ¤œç´¢çµæœ.xlsx",
                mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
            )
    except Exception as e:
        st.error(f"èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")

# ---------------------
# å•†å“ã‚¤ãƒ³ãƒãƒ¼ãƒˆãƒšãƒ¼ã‚¸
# ---------------------
elif page == "å•†å“ã‚¤ãƒ³ãƒãƒ¼ãƒˆ":
    st.title("ğŸ“¦ å•†å“ãƒã‚¹ã‚¿ï¼šExcelã‚¤ãƒ³ãƒãƒ¼ãƒˆã¨ã‚µã‚¤ã‚ºå±•é–‹")
    uploaded_file = st.file_uploader("Excelãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰", type=["xlsx"])
    if uploaded_file:
        df = pd.read_excel(uploaded_file, header=1)
        st.subheader("å…ƒãƒ‡ãƒ¼ã‚¿")
        st.dataframe(df)

        def expand_sizes(df):
            df = df.copy()
            df["ã‚µã‚¤ã‚º"] = df["ã‚µã‚¤ã‚º"].astype(str).str.replace("ã€", ",").str.split(",")
            df["ã‚µã‚¤ã‚º"] = df["ã‚µã‚¤ã‚º"].apply(lambda x: [s.strip() for s in x])
            return df.explode("ã‚µã‚¤ã‚º").reset_index(drop=True)

        expanded_df = expand_sizes(df)
        st.subheader("å±•é–‹å¾Œï¼ˆ1ã‚µã‚¤ã‚º1è¡Œï¼‰")
        st.dataframe(expanded_df)

                if st.button("Googleã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã«ä¿å­˜"):
                try:
                    try:
                        product_sheet = spreadsheet.worksheet("åŸºæº–IDãƒã‚¹ã‚¿")
                    except gspread.exceptions.WorksheetNotFound:
                        product_sheet = spreadsheet.add_worksheet(title="åŸºæº–IDãƒã‚¹ã‚¿", rows="100", cols="20")

                    try:
                        standard_sheet = spreadsheet.worksheet("åŸºæº–å€¤")
                    except gspread.exceptions.WorksheetNotFound:
                        standard_sheet = spreadsheet.add_worksheet(title="åŸºæº–å€¤", rows="100", cols="50")

                    product_existing = pd.DataFrame(product_sheet.get_all_records())
                    standard_existing = pd.DataFrame(standard_sheet.get_all_records())

                    updated_product = pd.concat([product_existing, product_df], ignore_index=True).drop_duplicates()
                    updated_standard = pd.concat([standard_existing, standard_df], ignore_index=True).drop_duplicates()

                    product_sheet.clear()
                    product_sheet.update([updated_product.columns.tolist()] + updated_product.values.tolist())

                    standard_sheet.clear()
                    standard_sheet.update([updated_standard.columns.tolist()] + updated_standard.values.tolist())

                    st.success("âœ… åŸºæº–å€¤ã‚’ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã«ä¿å­˜ã—ã¾ã—ãŸï¼")

                except Exception as e:
                    st.error(f"ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")

                
# ---------------------
# åŸºæº–å€¤ã‚¤ãƒ³ãƒãƒ¼ãƒˆãƒšãƒ¼ã‚¸
# ---------------------
elif page == "åŸºæº–å€¤ã‚¤ãƒ³ãƒãƒ¼ãƒˆ":
    st.title("ğŸ“ åŸºæº–å€¤ã‚¤ãƒ³ãƒãƒ¼ãƒˆï¼ˆç®¡ç†ç•ªå·ï¼ä»£è¡¨IDãƒ™ãƒ¼ã‚¹ï¼‰")

    uploaded_file = st.file_uploader("åŸºæº–å€¤Excelãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰", type=["xlsx"])

    if uploaded_file:
        try:
            # Excelã‹ã‚‰ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
            product_df = pd.read_excel(uploaded_file, sheet_name="å•†å“ãƒã‚¹ã‚¿")
            standard_df = pd.read_excel(uploaded_file, sheet_name="åŸºæº–ID")

            selected_pid = st.selectbox("å•†å“ç®¡ç†ç•ªå·ã‚’é¸æŠ", product_df["å•†å“ç®¡ç†ç•ªå·"].unique())

            if selected_pid:
                product_row = product_df[product_df["å•†å“ç®¡ç†ç•ªå·"] == selected_pid].iloc[0]
                base_id = product_row["åŸºæº–ID"]

                st.write(f"**å•†å“åï¼š** {product_row['å•†å“å']}ã€€ã€€**ã‚«ãƒ©ãƒ¼ï¼š** {product_row['ã‚«ãƒ©ãƒ¼']}")
                st.write(f"**åŸºæº–IDï¼š** {base_id}")

                filtered = standard_df[standard_df["åŸºæº–ID"] == base_id].drop(columns="åŸºæº–ID")
                filtered = filtered.set_index("ã‚µã‚¤ã‚º")

                st.markdown("### ğŸ“ ã“ã®å•†å“ã®ã‚µã‚¤ã‚ºåˆ¥ åŸºæº–æ¡å¯¸å€¤")
                st.dataframe(filtered, use_container_width=True)

            if st.button("Googleã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã«ä¿å­˜"):
                try:
                    # ã‚·ãƒ¼ãƒˆå–å¾—ï¼ˆãªã‘ã‚Œã°è‡ªå‹•ä½œæˆï¼‰
                    try:
                        product_sheet = spreadsheet.worksheet("åŸºæº–IDãƒã‚¹ã‚¿")
                    except gspread.exceptions.WorksheetNotFound:
                        product_sheet = spreadsheet.add_worksheet(title="åŸºæº–IDãƒã‚¹ã‚¿", rows="100", cols="20")

                    try:
                        standard_sheet = spreadsheet.worksheet("åŸºæº–å€¤")
                    except gspread.exceptions.WorksheetNotFound:
                        standard_sheet = spreadsheet.add_worksheet(title="åŸºæº–å€¤", rows="100", cols="50")

                    # ç¾åœ¨ã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
                    product_existing = pd.DataFrame(product_sheet.get_all_records())
                    standard_existing = pd.DataFrame(standard_sheet.get_all_records())

                    # æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ã‚’ãƒãƒ¼ã‚¸ï¼ˆé‡è¤‡æ’é™¤ï¼‰
                    updated_product = pd.concat([product_existing, product_df], ignore_index=True).drop_duplicates()
                    updated_standard = pd.concat([standard_existing, standard_df], ignore_index=True).drop_duplicates()

                    # Googleã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã¸åæ˜ 
                    product_sheet.clear()
                    product_sheet.update([updated_product.columns.tolist()] + updated_product.values.tolist())

                    standard_sheet.clear()
                    standard_sheet.update([updated_standard.columns.tolist()] + updated_standard.values.tolist())

                    st.success("âœ… åŸºæº–å€¤ã‚’ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã«ä¿å­˜ã—ã¾ã—ãŸï¼")

                except Exception as e:
                    st.error(f"ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")

        except Exception as e:
            st.error(f"èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")



            if selected_pid:
                product_row = product_df[product_df["å•†å“ç®¡ç†ç•ªå·"] == selected_pid].iloc[0]
                base_id = product_row["åŸºæº–å€¤"]

                st.write(f"**å•†å“åï¼š** {product_row['å•†å“å']}ã€€ã€€**ã‚«ãƒ©ãƒ¼ï¼š** {product_row['ã‚«ãƒ©ãƒ¼']}")
                st.write(f"**åŸºæº–IDï¼š** {base_id}")

                filtered = standard_df[standard_df["åŸºæº–ID"] == base_id].drop(columns="åŸºæº–ID")
                filtered = filtered.set_index("ã‚µã‚¤ã‚º")

                st.markdown("### ğŸ“ ã“ã®å•†å“ã®ã‚µã‚¤ã‚ºåˆ¥ åŸºæº–æ¡å¯¸å€¤")
                st.dataframe(filtered, use_container_width=True)
        except Exception as e:
            st.error(f"èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")

# ---------------------
# æ¡å¯¸ãƒ˜ãƒƒãƒ€ãƒ¼åˆæœŸåŒ–ãƒšãƒ¼ã‚¸ï¼ˆä¸¡æ–¹å¯¾å¿œï¼‰
# ---------------------
elif page == "æ¡å¯¸ãƒ˜ãƒƒãƒ€ãƒ¼åˆæœŸåŒ–":
    st.title("ğŸ“‹ æ¡å¯¸ã‚·ãƒ¼ãƒˆ ãƒ˜ãƒƒãƒ€ãƒ¼åˆæœŸåŒ–ï¼ˆâ€»ãƒ‡ãƒ¼ã‚¿ã¯æ®‹ã™ï¼‰")

    headers = ["æ—¥ä»˜", "å•†å“ç®¡ç†ç•ªå·", "ãƒ–ãƒ©ãƒ³ãƒ‰", "ã‚«ãƒ†ã‚´ãƒª", "å•†å“å", "ã‚«ãƒ©ãƒ¼", "ã‚µã‚¤ã‚º",
               "è‚©å¹…", "èƒ¸å¹…", "èƒ´å›²", "è¢–ä¸ˆ", "ç€ä¸ˆ", "è¥Ÿé«˜", "ã‚¦ã‚¨ã‚¹ãƒˆ", "è‚¡ä¸Š", "è‚¡ä¸‹",
               "ãƒ¯ã‚¿ãƒª", "è£¾å¹…", "å…¨é•·", "æœ€å¤§å¹…", "æ¨ªå¹…", "é ­å‘¨ã‚Š", "ãƒ„ãƒ", "é«˜ã•", "è£„ä¸ˆ", "ãƒ™ãƒ«ãƒˆå¹…", "å‰ä¸ˆ", "å¾Œä¸ˆ"]

    def reinitialize_sheet(sheet_name):
        try:
            sheet = spreadsheet.worksheet(sheet_name)
            all_data = sheet.get_all_values()[1:]  # ãƒ‡ãƒ¼ã‚¿éƒ¨åˆ†ï¼ˆ2è¡Œç›®ä»¥é™ï¼‰

            sheet.clear()
            sheet.append_row(headers)

            if all_data:
                normalized = [row + [''] * (len(headers) - len(row)) for row in all_data]
                sheet.append_rows(normalized)
            st.success(f"âœ… ã€{sheet_name}ã€ã®ãƒ˜ãƒƒãƒ€ãƒ¼ã‚’åˆæœŸåŒ–ã—ã¾ã—ãŸï¼")
        except Exception as e:
            st.error(f"ã€{sheet_name}ã€ã®å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")

    if st.button("ğŸ§¼ æ¡å¯¸çµæœã‚·ãƒ¼ãƒˆã®åˆæœŸåŒ–"):
        reinitialize_sheet("æ¡å¯¸çµæœ")

    if st.button("ğŸ§¼ æ¡å¯¸ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ã‚·ãƒ¼ãƒˆã®åˆæœŸåŒ–"):
        reinitialize_sheet("æ¡å¯¸ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–")

# ---------------------
# ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ç®¡ç†ãƒšãƒ¼ã‚¸ï¼ˆ30æ—¥è¶…ãƒ‡ãƒ¼ã‚¿ç§»å‹•ï¼‰
# ---------------------
elif page == "ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ç®¡ç†":
    st.title("ğŸ—ƒï¸ æ¡å¯¸ãƒ‡ãƒ¼ã‚¿ã®ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ç®¡ç†")

    def parse_date_flexibly(date_str):
        for fmt in ("%Y-%m-%d", "%Y/%m/%d", "%Y.%m.%d"):
            try:
                return datetime.strptime(date_str.strip(), fmt)
            except:
                continue
        return None  # ä¸æ­£ãªæ—¥ä»˜ãªã‚‰ None

    if st.button("ğŸ“¦ 30æ—¥ä»¥ä¸Šå‰ã®æ¡å¯¸çµæœã‚’ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ã«ç§»å‹•"):
        try:
            result_ws = spreadsheet.worksheet("æ¡å¯¸çµæœ")
            archive_ws = spreadsheet.worksheet("æ¡å¯¸ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–")
            values = result_ws.get_all_values()
            headers = values[0]
            rows = values[1:]

            old_rows = []
            recent_rows = []
            today = datetime.now()

            for row in rows:
                row += [''] * (len(headers) - len(row))
                parsed_date = parse_date_flexibly(row[0])

                if parsed_date and (today - parsed_date).days > 30:
                    old_rows.append(row)
                else:
                    recent_rows.append(row)

            if old_rows:
                archive_data = archive_ws.get_all_values()
                if not archive_data:
                    archive_ws.append_row(headers)
                archive_ws.append_rows(old_rows)

            result_ws.clear()
            result_ws.append_row(headers)
            if recent_rows:
                result_ws.append_rows(recent_rows)

            st.success(f"âœ… {len(old_rows)} ä»¶ã‚’ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ã«ç§»å‹•ã—ã¾ã—ãŸï¼")
        except Exception as e:
            st.error(f"ã‚¨ãƒ©ãƒ¼: {e}")
